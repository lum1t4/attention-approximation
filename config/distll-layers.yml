params:
  model_config_path: data/MobileLLM/config.json
  model_weights_path: data/MobileLLM/model.safetensors
  data_path: data/minipile
  checkpoint_dir: checkpoints
  device: cuda

  # Training settings
  max_steps: 10000000
  batch_size: 18 # (Rank, Optimal BS) = (128, 18), (64, 16), (32, 32), (16, 64)
  seq_length: 512
  grad_accum_steps: 4
  lr: 0.001
  min_lr: 0.00001
  weight_decay: 0.00001
  warmup_steps: 100
  grad_clip: 1.0

  # Logging & saving
  save_interval: 10
  val_interval: 250
  val_batches: 10
  val: "validation"

  # Distillation-specific
  factorization_rank: 16
  layer_sharing: true
  # Tracker
  tracker: null
  project: "attention-approximation"
